# ğŸ“‘ 4.6 â€” Health checks

Cette Ã©tape est le **dernier filet de sÃ©curitÃ©** du workflow : elle vÃ©rifie que les services clÃ©s (Prometheus, Loki, Grafana) sont bien **dÃ©marrÃ©s et accessibles** aprÃ¨s le dÃ©ploiement.

---

## ğŸ”¹ Le step dans le workflow

```yaml
# âœ… Sanity checks rapides
- name: Health checks
  uses: appleboy/ssh-action@v1.0.0
  with:
    host: ${{ vars.SSH_HOST }}
    username: ${{ vars.SSH_USER }}
    password: ${{ secrets.SSH_PASSWORD }}
    port: 22
    script: |
      set -e
      echo "Prometheus:" && curl -sf http://localhost:9090/-/ready && echo " OK"
      echo "Loki:"       && curl -sf http://localhost:3100/ready   && echo " OK"
      echo "Grafana:"    && curl -sf http://localhost:3000/login   && echo " OK"
```

---

## ğŸ” DÃ©composition ligne par ligne

### SÃ©curisation

* `set -e` : stoppe le script **au premier Ã©chec**.
  ğŸ‘‰ Si un seul service nâ€™est pas accessible, le job Ã©choue immÃ©diatement.

---

### VÃ©rification Prometheus

```bash
curl -sf http://localhost:9090/-/ready && echo " OK"
```

* Endpoint `/ready` : fourni par Prometheus pour indiquer quâ€™il est **opÃ©rationnel**.
* `-s` = silencieux, `-f` = Ã©choue si HTTP != 200.
* Si Ã§a passe â†’ affiche `OK`.
  ğŸ‘‰ Permet de savoir si Prometheus est prÃªt Ã  accepter des requÃªtes.

---

### VÃ©rification Loki

```bash
curl -sf http://localhost:3100/ready && echo " OK"
```

* Loki expose Ã©galement un endpoint `/ready`.
* VÃ©rifie que lâ€™**ingestion des logs** est fonctionnelle.
  ğŸ‘‰ Sans Ã§a, Grafana verrait bien la source, mais aucun log ne serait stockÃ©.

---

### VÃ©rification Grafana

```bash
curl -sf http://localhost:3000/login && echo " OK"
```

* Ici pas de `/ready`, donc on teste la page `/login`.
* Si Grafana rÃ©pond avec HTTP 200, câ€™est quâ€™il est bien dÃ©marrÃ© et son serveur web tourne.
  ğŸ‘‰ Câ€™est une vÃ©rif basique mais suffisante.

---

## ğŸ§  Ce quâ€™on pourrait aussi ajouter

* **Healthcheck pour Fluent Bit**
  â†’ pas dâ€™API native, mais on peut vÃ©rifier si le port `2020` (HTTP server de Fluent Bit) rÃ©pond.

  ```bash
  curl -sf http://localhost:2020 || echo "Fluent Bit KO"
  ```

* **Tester lâ€™authentification Grafana** (si besoin)
  â†’ simuler un login avec `curl -u admin:password`.
  Permet de vÃ©rifier que les **variables dâ€™environnement sensibles** (user/password) sont bien prises en compte.

* **Sanity check des dashboards**
  â†’ appeler lâ€™API Grafana `/api/dashboards/home` avec un token admin pour confirmer que la DB et les dashboards sont chargÃ©s.

* **VÃ©rifier les volumes**
  â†’ sâ€™assurer que les rÃ©pertoires de persistance (Grafana, Prometheus, Loki) existent encore aprÃ¨s un `down/up`.

* **Alertes en auto-test**
  â†’ envoyer une fausse alerte (`amtool` pour Alertmanager, ou `curl` vers Grafana Alerting) afin de valider que la chaÃ®ne de notifications email fonctionne.

---

## âœ… TL;DR

* Ce step est un **test de fumÃ©e** (smoke test) : est-ce que les services critiques sont **UP et rÃ©pondent correctement** ?
* Si lâ€™un dâ€™eux Ã©choue â†’ le workflow sâ€™arrÃªte â†’ pas de dÃ©ploiement â€œzombieâ€.
* On pourrait lâ€™enrichir avec :

    * VÃ©rif Fluent Bit
    * VÃ©rif login Grafana
    * VÃ©rif API Grafana dashboards/alertes
    * Test notification email

ğŸ‘‰ Câ€™est la **garantie minimale** que ton monitoring Sentinel est bien en vie aprÃ¨s chaque push.

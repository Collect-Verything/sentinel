## UtilitÃ© grafana



## ğŸ” 1. **Exploration ad hoc**

Ouvrir lâ€™onglet â€œExploreâ€ de Grafana, taper une requÃªte LogQL et naviguer dans les logs.  
Câ€™est utile pour :

- **dÃ©boguer** un problÃ¨me ponctuel (â€œun client me dit quâ€™il a une 500 Ã  14h12, voyons les logsâ€).
- **chercher des patterns** (ex: â€œquels endpoints provoquent le plus dâ€™erreurs 404 ?â€).


---

## ğŸ“Š 2. **Dashboards**

ğŸ‘‰ Les gens crÃ©ent des **dashboards visuels** dans Grafana Ã  partir des logs.  
Exemple :

- Nombre de requÃªtes par seconde (RPS).
- Taux dâ€™erreurs 4xx / 5xx par endpoint.
- Histogramme des temps de rÃ©ponse (si tu logs les temps).
- Logs systÃ¨me (auth, SSH, etc.) centralisÃ©s pour sÃ©curitÃ©.

Ã‡a permet de voir **en un coup dâ€™Å“il** si lâ€™app va bien sans lire 1000 lignes de logs.


## ğŸ§© Ã‰tape 1 : Identifier les infos quâ€™on veut

Avec ta config actuelle, Fluent Bit envoie :

- **Logs Nginx** (via `/dev/stdout` du container)
- **Logs systÃ¨me** (`/var/log/syslog`, `/var/log/auth.log`)


ğŸ‘‰ Donc sur Grafana/Loki on a dÃ©jÃ  des `labels` comme :

- `job="nginx"` (dÃ©fini dans `fluent-bit.conf`)
- `host="..."` (nom de ta machine)


Et dans les logs Nginx, tu as normalement des lignes du style :

```
172.17.0.1 - - [03/Sep/2025:10:52:10 +0000] "GET /api/v1/products HTTP/1.1" 200 123 "-" "Mozilla/5.0"
```

Ã‡a veut dire quâ€™on peut extraire :

- **status code** (200, 404, 500â€¦)
- **endpoint** (`/api/v1/products`)
- **volume de trafic**


---

## ğŸ§© Ã‰tape 2 : RequÃªtes LogQL utiles

Tu vas en avoir besoin pour construire les panels :

1. **Nombre total de requÃªtes**


```logql
count_over_time({job="nginx"}[1m])
```

2. **Taux dâ€™erreurs 5xx**


```logql
sum(rate({job="nginx"} | regexp "(?P<status>[0-9]{3})" | unwrap status | status =~ "5.." [5m]))
```

3. **Top endpoints appelÃ©s**


```logql
topk(5, count_over_time({job="nginx"} | regexp "\"[A-Z]+ (?P<endpoint>[^ ]+) " | unwrap endpoint [5m]))
```

4. **Logs bruts (debug)**


```logql
{job="nginx"}
```

---


---

### ğŸ”¹ 2. Logs systÃ¨me (depuis Fluent Bit)

GrÃ¢ce Ã  `system.syslog` et `system.auth` :

- **Auth logs (tentatives SSH)** :

    ```logql
    count_over_time({job="nginx", tag="system.auth"} |= "Failed password"[5m])
    ```

- **Erreurs critiques systÃ¨me** :

    ```logql
    count_over_time({job="nginx", tag="system.syslog"} |= "error"[5m])
    ```





# ğŸ“ Note : Relation entre Fluent Bit `[INPUT]` et les volumes Docker

## 1. Comment Fluent Bit lit des fichiers de log ?

- Fluent Bit est lancÃ© **dans un conteneur Docker**.
- Comme tout conteneur, il est **isolÃ©** de lâ€™hÃ´te : par dÃ©faut, il ne "voit" pas les fichiers de ton serveur (ex. `/var/log/syslog`).
- Quand tu Ã©cris dans `fluent-bit.conf` :

```ini
[INPUT]
    Name tail
    Path /var/log/syslog
    Tag  system.syslog
```

- `Name tail` â†’ on lit un fichier en continu
- `Path` â†’ chemin exact du fichier (qui doit exister grÃ¢ce au volume)
- `Tag` â†’ identifiant du flux (ex. `system.syslog`)

ğŸ‘‰ Ã‡a veut dire : **"Fluent Bit doit lire `/var/log/syslog`"**.

âš ï¸ Mais si ce fichier nâ€™existe **pas dans le conteneur**, Fluent Bit plante ou ne lit rien.


Attention chaque input doit posseder un output :

Chaque `[OUTPUT]` envoie les logs vers une destination (ici Loki).  
**RÃ¨gle dâ€™or** : si tu veux distinguer plusieurs types de logs dans Grafana, tu dois crÃ©er **un output par type dâ€™input/tag**.

```json
[OUTPUT]
    Name        loki
    Match       nginx.*
    Host        82.165.92.40
    Port        3100
    Labels      job=nginx,host=${HOSTNAME}
    line_format json

[OUTPUT]
    Name        loki
    Match       system.*
    Host        82.165.92.40
    Port        3100
    Labels      job=system,host=${HOSTNAME}
    line_format json


```

- `Match` â†’ indique quels tags dâ€™INPUT sont envoyÃ©s (ex. `nginx.*` ou `system.*`)
- `Labels` â†’ ajoute un label `job=xxx` pour les retrouver dans Grafana


### ğŸ“ Mini Note : `Match` dans Fluent Bit

- Chaque `[OUTPUT]` de Fluent Bit a une directive `Match` qui dit **quels logs (tags) vont Ãªtre envoyÃ©s vers cette sortie**.

- **Tags** :
    - Chaque `[INPUT]` attribue un `Tag` (ex: `nginx.access`, `system.syslog`).
    - Câ€™est ce `Tag` que `Match` utilise pour savoir quoi router.

- **RÃ¨gles** :
    - `Match *` â†’ prend **tous** les logs (attention, Ã§a peut â€œavalerâ€ les autres).
    - `Match nginx.*` â†’ prend seulement les logs dont le tag commence par `nginx.`
    - `Match system.syslog` â†’ prend seulement ce tag prÃ©cis.

- **Bonne pratique** :
    - Ã‰viter `Match *` sauf pour un **catch-all** en dernier recours.
    - CrÃ©er un `[OUTPUT]` par â€œfamille de logsâ€ (nginx, system, auth, misc).
    - Ã‡a garde les flux **propres et sÃ©parÃ©s** cÃ´tÃ© Loki/Grafana.


---

## 2. Le rÃ´le des volumes dans `docker-compose.yml`

Un volume permet de **partager des fichiers de ton hÃ´te (serveur)** avec un conteneur.

Exemple :
```yaml
volumes:
  - /var/log:/var/log:ro
```

ğŸ‘‰ Cela fait apparaÃ®tre **le rÃ©pertoire `/var/log` de ton serveur** dans **le `/var/log` du conteneur**.
- Ainsi, quand Fluent Bit cherche `/var/log/syslog`, il trouve bien le vrai fichier de ton hÃ´te.
- Lâ€™option `:ro` (read-only) est importante : Fluent Bit lit les logs mais ne peut pas les modifier.

Sans ce volume : `/var/log/syslog` dans le conteneur est vide ou inexistant â†’ Fluent Bit ne lit rien.

---

## 3. Exemple concret

### docker-compose.yml :

```yaml
fluent-bit:
  image: fluent/fluent-bit:latest
  container_name: fluent-bit
  volumes:
    - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf
    - /var/lib/docker/containers:/var/lib/docker/containers:ro   # logs Docker JSON
    - /var/run/docker.sock:/var/run/docker.sock                  # mÃ©tadonnÃ©es Docker
    - /var/log:/var/log:ro                                       # logs systÃ¨me
  ports:
    - "2020:2020"
```

### fluent-bit.conf :

```ini
# Logs systÃ¨me
[INPUT]
    Name tail
    Path /var/log/syslog
    Tag  system.syslog

[INPUT]
    Name tail
    Path /var/log/auth.log
    Tag  system.auth

# Logs Docker (Nginx, client, etc.)
[INPUT]
    Name tail
    Path /var/lib/docker/containers/*/*-json.log
    Tag  docker.*
    Parser docker
```

---

## 4. DÃ©roulÃ© complet (chaÃ®ne dâ€™acheminement)

1. **Ton serveur Ã©crit des logs** :
    - `/var/log/syslog` : logs systÃ¨me
    - `/var/log/auth.log` : authentifications
    - `/var/lib/docker/containers/...-json.log` : logs des conteneurs (stdout/stderr)

2. **Docker volume monte ces chemins** dans le conteneur Fluent Bit.
3. **Fluent Bit lit ces fichiers** grÃ¢ce aux `[INPUT]` de `fluent-bit.conf`.
4. Fluent Bit **parse** les logs (si tu as dÃ©fini un `Parser` comme `docker`).
5. Fluent Bit envoie ces logs vers un `[OUTPUT]` (dans ton cas Loki).


---

## 5. Pourquoi câ€™est indispensable ?

- Sans volume : le conteneur est isolÃ© â†’ `/var/log/syslog` est introuvable â†’ pas de logs collectÃ©s.
- Avec volume : Fluent Bit voit les vrais fichiers du serveur â†’ collecte rÃ©ussie.


---

ğŸ‘‰ RÃ©sumÃ© :  
**Le `[INPUT]` dit Ã  Fluent Bit _quel chemin lire_.  
Le volume Docker fait en sorte que ce chemin existe dans le conteneur.  
Sans volume, lâ€™`INPUT` pointe dans le vide.**


```json
[Serveur Linux]
   /var/log/syslog
   /var/log/auth.log
   /var/lib/docker/containers/...-json.log
        |
        | (montÃ©s en volumes dans docker-compose)
        v
[Fluent Bit container]
   INPUT (tail, docker...) + TAG
        v
   OUTPUT (Loki) + LABEL (job=nginx, job=system, etc.)
        v
[Loki + Grafana]
   RequÃªtes LogQL : {job="nginx"} , {job="system"} ...

```




---

### ğŸ”¹ 3. Metrics serveurs (Node Exporter)

Comme tu as **node-exporter**, on peut rajouter :

- **CPU usage** :

    ```promql
    100 - (avg by (instance)(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
    ```

- **RAM usage** :

    ```promql
    node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
    ```

- **Disk usage** :

    ```promql
    node_filesystem_size_bytes - node_filesystem_free_bytes
    ```


ğŸ‘‰ Ces panels tu les mets dans une **section â€œInfrastructureâ€** du dashboard.

---

### ğŸ”¹ 4. Alertes possibles

- **HTTP 500 > 5 en 1m**

- **Auth SSH Ã©chouÃ© > 10 en 5m**

- **CPU > 80% pendant 2m**


---

ğŸ‘‰ Donc ton dashboard pourrait Ãªtre structurÃ© comme Ã§a :

1. **Overview**

    - Nombre total de requÃªtes (nginx)

    - Erreurs 4xx / 5xx

    - Auth SSH Ã©chouÃ©es

2. **Traffic**

    - Top endpoints

    - Graph codes HTTP

3. **System**

    - Syslog errors

    - Auth logs

4. **Infrastructure**

    - CPU / RAM / Disk

    - Uptime du node




---

## ğŸš¨ 3. **Alerting**

ğŸ‘‰ GrÃ¢ce Ã  Loki + Alertmanager (ou Grafana Alerting), on peut dÃ©finir des **alertes automatiques** :

- Si >1% des requÃªtes sont des 500 sur 5 minutes â†’ alerte Slack.

- Si un utilisateur tente 20 logins ratÃ©s en 1 minute â†’ alerte sÃ©curitÃ©.

- Si ton service ne gÃ©nÃ¨re plus de logs â†’ alerte (le service est peut-Ãªtre down).


Pour le smail il faut creer une alert, et ajouter un mail dans la config contact point: creer l'alerte avec une query ciblÃ© et cofigurer le smtp de grafana dans le docker compose ou dans la config, ici docker compose=

Parfait âœ… reprenons doucement, Ã©tape par Ã©tape, en utilisant **Gmail** comme SMTP (câ€™est plus simple que Ionos pour tester).

---

## 1. VÃ©rifier si Grafana tourne en Docker

```bash
docker ps | grep grafana
```

ğŸ‘‰ Si tu vois `grafana/grafana`, alors Grafana est bien en container.  
Dans ce cas, sa config (`grafana.ini`) est **Ã  lâ€™intÃ©rieur** du container, pas sur ton serveur.

---

## ğŸ—‚ï¸ 4. **Centralisation multi-services**

ğŸ‘‰ Quand tu as plusieurs apps/microservices, tu envoies **tous les logs** (front, back, base, infra) dans Loki.  
Avantage : tu peux **corrÃ©ler** :

- voir quâ€™un appel frontend a dÃ©clenchÃ© un appel backend â†’ qui a Ã©chouÃ© â†’ erreur DB.

- suivre une requÃªte avec un `trace_id` unique dans tous les services.


---

## ğŸ” 5. **Audit & sÃ©curitÃ©**

ğŸ‘‰ Beaucoup dâ€™Ã©quipes exploitent les logs pour la **compliance** ou la **cybersÃ©curitÃ©** :

- garder un historique centralisÃ© (mÃªme si une machine crashe, les logs sont sauvegardÃ©s).

- dÃ©tecter des comportements anormaux (ex: brute force SSH, injection SQL, spam).


---

## ğŸ§‘â€ğŸ’» ConcrÃ¨tement au quotidien

- Les devs â†’ vont dans Grafana â€œExploreâ€ pour **debugger**.

- Les ops/devops â†’ utilisent des **dashboards + alertes** pour surveiller lâ€™Ã©tat global.

- Les responsables sÃ©curitÃ© â†’ exploitent les logs centralisÃ©s pour **auditer**.


